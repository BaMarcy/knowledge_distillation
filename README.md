 ## Knowledge distillation in PyTorch
 
[**ğŸ’¾ DATASET**](https://github.com/joojs/fairface) **|** [**ğŸ’» Jupyter Notebook**](https://jupyter.org/install) **|** [**ğŸ”¥ PyTorch**](https://pytorch.org/get-started/locally/) **|** **ğŸ‘ CNN** **|** **ğŸ’ªğŸ½ CPU/GPU** **|**  [**ğŸ”— LinkedIn**](https://www.linkedin.com/in/marcellbalogh) ğŸ‘ˆğŸ½ 

[![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://bamarcy-age-gender-streamlit-streamlit-app-fr1omm.streamlitapp.com/)

#### ğŸ” Description
<p align="justify">Knowledge distillation is a model compression technique in machine learning where a smaller model (student) is trained to replicate the behavior of a larger, more complex model (teacher). The idea is to transfer the knowledge and generalization capabilities of the teacher model to the smaller student model.</p>

<p align="center">
  <img src="test.PNG">
</p>

---
#### â˜‘ï¸ Prerequisites
- Python3
- See [requirements.txt](requirements.txt) for required packages

###### âš™ï¸ Installation
```html
   git clone https://github.com/BaMarcy/age_gender_predictor
```
```html
   pip install -r requirements.txt
```
---
#### ğŸ› ï¸ Train + ğŸ’Š Predict
###### âš™ï¸ Run
```html
   jupyter notebook knowledge_distillation.ipynb
```
---

---
#### ğŸ“‰ Tensorboard

![](tensorboard.png)
---
#### â˜‘ï¸ TODO
- [] sth
---
