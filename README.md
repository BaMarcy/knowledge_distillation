 ## Knowledge distillation in PyTorch
 
[**💾 DATASET**](https://github.com/joojs/fairface) **|** [**💻 Jupyter Notebook**](https://jupyter.org/install) **|** [**🔥 PyTorch**](https://pytorch.org/get-started/locally/) **|** **👁 CNN** **|** **💪🏽 CPU/GPU** **|**  [**🔗 LinkedIn**](https://www.linkedin.com/in/marcellbalogh) 👈🏽 

[![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://bamarcy-age-gender-streamlit-streamlit-app-fr1omm.streamlitapp.com/)

#### 🔍 Description
<p align="justify">Knowledge distillation is a model compression technique in machine learning where a smaller model (student) is trained to replicate the behavior of a larger, more complex model (teacher). The idea is to transfer the knowledge and generalization capabilities of the teacher model to the smaller student model.</p>

<p align="center">
  <img src="test.PNG">
</p>

---
#### ☑️ Prerequisites
- Python3
- See [requirements.txt](requirements.txt) for required packages

###### ⚙️ Installation
```html
   git clone https://github.com/BaMarcy/age_gender_predictor
```
```html
   pip install -r requirements.txt
```
---
#### 🛠️ Train + 💊 Predict
###### ⚙️ Run
```html
   jupyter notebook knowledge_distillation.ipynb
```
---

---
#### 📉 Tensorboard

![](tensorboard.png)
---
#### ☑️ TODO
- [] sth
---
